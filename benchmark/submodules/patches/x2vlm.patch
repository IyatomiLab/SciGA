diff --git a/Grounding_bbox.py b/Grounding_bbox.py
index 433925d..4574337 100644
--- a/Grounding_bbox.py
+++ b/Grounding_bbox.py
@@ -13,18 +13,18 @@ import torch
 import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
-import utils
-from utils.checkpointer import Checkpointer
+import benchmark.submodules.x2vlm.utils as utils
+from benchmark.submodules.x2vlm.utils.checkpointer import Checkpointer
 
-from dataset import create_dataset, create_sampler, create_loader, build_tokenizer
-from dataset.utils import collect_tensor_result, grounding_eval_bbox, grounding_eval_bbox_vlue
+from benchmark.submodules.x2vlm.dataset import create_dataset, create_sampler, create_loader, build_tokenizer
+from benchmark.submodules.x2vlm.dataset.utils import collect_tensor_result, grounding_eval_bbox, grounding_eval_bbox_vlue
 
-from models.model_grounding import XVLMForGrounding
+from benchmark.submodules.x2vlm.models.model_grounding import XVLMForGrounding
 
-from optim import create_optimizer
-from refTools.refer_python3 import REFER
-from scheduler import create_scheduler
-from utils.hdfs_io import hmkdir, hcopy, hexists
+from benchmark.submodules.x2vlm.optim import create_optimizer
+from benchmark.submodules.x2vlm.refTools.refer_python3 import REFER
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.utils.hdfs_io import hmkdir, hcopy, hexists
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler, config):
diff --git a/MARVL.py b/MARVL.py
index 6c8edc1..dc91d97 100644
--- a/MARVL.py
+++ b/MARVL.py
@@ -17,10 +17,10 @@ import torch
 import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
-import utils
+import benchmark.submodules.x2vlm.utils as utils
 from dataset import create_dataset, create_sampler, create_loader, build_tokenizer
-from scheduler import create_scheduler
-from optim import create_optimizer
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler):
@@ -131,7 +131,7 @@ def main(args, config):
                                             num_workers=[4], is_trains=[False], collate_fns=[None])[0]
 
     print("Creating model")
-    from models.model_classification import XVLMPlusForMARVL
+    from benchmark.submodules.x2vlm.models.model_classification import XVLMPlusForMARVL
     model = XVLMPlusForMARVL(config=config)
     model.load_pretrained(args.checkpoint, config, is_eval=args.evaluate)
     model = model.to(device)
diff --git a/NLVR.py b/NLVR.py
index 5a2e74c..5b4000d 100644
--- a/NLVR.py
+++ b/NLVR.py
@@ -17,15 +17,15 @@ import torch
 import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
-from models.model_classification import XVLMForNLVR
+from benchmark.submodules.x2vlm.models.model_classification import XVLMForNLVR
 
-import utils
-from utils.checkpointer import Checkpointer
-from utils.hdfs_io import hmkdir
+import benchmark.submodules.x2vlm.utils as utils
+from benchmark.submodules.x2vlm.utils.checkpointer import Checkpointer
+from benchmark.submodules.x2vlm.utils.hdfs_io import hmkdir
 
-from dataset import create_dataset, create_sampler, create_loader, build_tokenizer
-from scheduler import create_scheduler
-from optim import create_optimizer
+from benchmark.submodules.x2vlm.dataset import create_dataset, create_sampler, create_loader, build_tokenizer
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler):
diff --git a/Pretrain.py b/Pretrain.py
index 9c2b242..8728275 100644
--- a/Pretrain.py
+++ b/Pretrain.py
@@ -20,14 +20,14 @@ import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 from torch.optim import Optimizer
 
-import utils
-from dataset import create_dataset
-from scheduler import create_scheduler
-from optim import create_optimizer
+import benchmark.submodules.x2vlm.utils as utils
+from benchmark.submodules.x2vlm.dataset import create_dataset
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
-from utils.checkpointer import Checkpointer
-from utils.hdfs_io import hmkdir, hcopy
-from accelerators.apex_ddp_accelerator import ApexDDPAccelerator
+from benchmark.submodules.x2vlm.utils.checkpointer import Checkpointer
+from benchmark.submodules.x2vlm.utils.hdfs_io import hmkdir, hcopy
+from benchmark.submodules.x2vlm.accelerators.apex_ddp_accelerator import ApexDDPAccelerator
 
 
 def reinit_scheduler_properties_mysched(optimizer: Optimizer, scheduler, cfg) -> None:
@@ -519,7 +519,7 @@ def main(args, config):
     print(f"Creating model {config.get('model_type', 'XVLM')}", flush=True)
     if config.get('model_type', ''):
         if config['model_type'] == 'XVLMPlus':
-            from models.model_pretrain import XVLMPlus
+            from benchmark.submodules.x2vlm.models.model_pretrain import XVLMPlus
 
             assert os.path.exists(args.checkpoint)
             model = XVLMPlus(config=config, load_text_params=False, load_vision_params=False, load_cross_params=False, pretraining=False)
@@ -531,7 +531,7 @@ def main(args, config):
             model.load_pretrained(args.checkpoint, config, text_ckpt_rpath=text_ckpt_rpath)
 
         elif config['model_type'] == 'CrossViewLM':
-            from models.model_pretrain import CrossViewLM
+            from benchmark.submodules.x2vlm.models.model_pretrain import CrossViewLM
 
             assert os.path.exists(args.checkpoint)
             model = CrossViewLM(config=config, load_text_params=False, load_vision_params=False, load_cross_params=False,
@@ -548,7 +548,7 @@ def main(args, config):
 
     else:
 
-        from models.model_pretrain import XVLM
+        from benchmark.submodules.x2vlm.models.model_pretrain import XVLM
 
         if os.path.exists(args.checkpoint):  # for domain pre-training
             model = XVLM(config=config, load_text_params=False, load_vision_params=False, pretraining=False)
diff --git a/Retrieval.py b/Retrieval.py
index 6df8aca..36e9f5e 100644
--- a/Retrieval.py
+++ b/Retrieval.py
@@ -17,13 +17,13 @@ import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
 
-import utils
-from utils.checkpointer import Checkpointer
-from utils.hdfs_io import hmkdir
+import benchmark.submodules.x2vlm.utils as utils
+from benchmark.submodules.x2vlm.utils.checkpointer import Checkpointer
+from benchmark.submodules.x2vlm.utils.hdfs_io import hmkdir
 
-from dataset import create_dataset, create_sampler, create_loader, build_tokenizer
-from scheduler import create_scheduler
-from optim import create_optimizer
+from benchmark.submodules.x2vlm.dataset import create_dataset, create_sampler, create_loader, build_tokenizer
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler, config):
@@ -241,10 +241,10 @@ def main(args, config):
     print(f"Creating model", flush=True)
     if args.text2video:
         print("Creating text2video model", flush=True)
-        from models.model_retrieval import XVLMForT2V
+        from benchmark.submodules.x2vlm.models.model_retrieval import XVLMForT2V
         model = XVLMForT2V(config=config)
     else:
-        from models.model_retrieval import XVLMForRetrieval
+        from benchmark.submodules.x2vlm.models.model_retrieval import XVLMForRetrieval
         model = XVLMForRetrieval(config=config)
 
     model.load_pretrained(args.checkpoint, config, is_eval=args.evaluate)
diff --git a/VQA.py b/VQA.py
index bc58f9b..c88b804 100644
--- a/VQA.py
+++ b/VQA.py
@@ -13,15 +13,15 @@ import torch
 import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
-import utils
-from utils.checkpointer import Checkpointer
-from utils.hdfs_io import hmkdir, hexists
+import benchmark.submodules.x2vlm.utils as utils
+from benchmark.submodules.x2vlm.utils.checkpointer import Checkpointer
+from benchmark.submodules.x2vlm.utils.hdfs_io import hmkdir, hexists
 
-from dataset.utils import collect_result
-from dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn, build_tokenizer
+from benchmark.submodules.x2vlm.dataset.utils import collect_result
+from benchmark.submodules.x2vlm.dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn, build_tokenizer
 
-from scheduler import create_scheduler
-from optim import create_optimizer
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler, config):
@@ -148,7 +148,7 @@ def main(args, config):
     config['eos'] = vqa_test_dataset.eos_token
 
     print(f"Creating model", flush=True)
-    from models.model_generation import XVLMForVQA
+    from benchmark.submodules.x2vlm.models.model_generation import XVLMForVQA
     model = XVLMForVQA(config=config)
     model.load_pretrained(args.checkpoint, config, is_eval=args.evaluate or args.load_vqa_pretrain)
     model = model.to(device)
diff --git a/VQA_msrvtt.py b/VQA_msrvtt.py
index 588e3e1..cb9da16 100644
--- a/VQA_msrvtt.py
+++ b/VQA_msrvtt.py
@@ -18,17 +18,17 @@ import torch
 import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
-from models.model_classification import XVLMForClassification
+from benchmark.submodules.x2vlm.models.model_classification import XVLMForClassification
 
-import utils
-from utils.checkpointer import Checkpointer
-from utils.hdfs_io import hmkdir, hexists, hcopy
+import benchmark.submodules.x2vlm.utils as utils
+from benchmark.submodules.x2vlm.utils.checkpointer import Checkpointer
+from benchmark.submodules.x2vlm.utils.hdfs_io import hmkdir, hexists, hcopy
 
-from dataset.utils import collect_result
-from dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn, build_tokenizer
+from benchmark.submodules.x2vlm.dataset.utils import collect_result
+from benchmark.submodules.x2vlm.dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn, build_tokenizer
 
-from scheduler import create_scheduler
-from optim import create_optimizer
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler, config):
diff --git a/VQA_msvd.py b/VQA_msvd.py
index e6c406a..760ea61 100644
--- a/VQA_msvd.py
+++ b/VQA_msvd.py
@@ -18,17 +18,17 @@ import torch
 import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
-from models.model_classification import XVLMForClassification
+from benchmark.submodules.x2vlm.models.model_classification import XVLMForClassification
 
-import utils
-from utils.checkpointer import Checkpointer
-from utils.hdfs_io import hmkdir, hexists, hcopy
+import benchmark.submodules.x2vlm.utils as utils
+from benchmark.submodules.x2vlm.utils.checkpointer import Checkpointer
+from benchmark.submodules.x2vlm.utils.hdfs_io import hmkdir, hexists, hcopy
 
-from dataset.utils import collect_result
-from dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn, build_tokenizer
+from benchmark.submodules.x2vlm.dataset.utils import collect_result
+from benchmark.submodules.x2vlm.dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn, build_tokenizer
 
-from scheduler import create_scheduler
-from optim import create_optimizer
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler, config):
diff --git a/WIT.py b/WIT.py
index 384e61f..84426be 100644
--- a/WIT.py
+++ b/WIT.py
@@ -16,11 +16,11 @@ import torch.nn.functional as F
 import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
-import utils
-from utils.hdfs_io import hopen, hexists, hmkdir
+import benchmark.submodules.x2vlm.utils as utils
+from benchmark.submodules.x2vlm.utils.hdfs_io import hopen, hexists, hmkdir
 from dataset import create_dataset, create_sampler, create_loader, build_tokenizer
-from scheduler import create_scheduler
-from optim import create_optimizer
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler, config):
@@ -248,7 +248,7 @@ def main(args, config):
                                             num_workers=[4], is_trains=[False], collate_fns=[None])[0]
 
     print("Creating model", flush=True)
-    from models.model_retrieval import XVLMPlusForRetrieval
+    from benchmark.submodules.x2vlm.models.model_retrieval import XVLMPlusForRetrieval
     model = XVLMPlusForRetrieval(config=config)
     model.load_pretrained(args.checkpoint, config, is_eval=args.evaluate)
     model = model.to(device)
diff --git a/XGQA.py b/XGQA.py
index cf3b04a..21cb44e 100644
--- a/XGQA.py
+++ b/XGQA.py
@@ -13,15 +13,15 @@ import torch
 import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
-import utils
-from utils.checkpointer import Checkpointer
-from utils.hdfs_io import hmkdir, hexists
+import benchmark.submodules.x2vlm.utils as utils
+from benchmark.submodules.x2vlm.utils.checkpointer import Checkpointer
+from benchmark.submodules.x2vlm.utils.hdfs_io import hmkdir, hexists
 
 from dataset.utils import collect_result
 from dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn, build_tokenizer
 
-from scheduler import create_scheduler
-from optim import create_optimizer
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler, config):
@@ -178,7 +178,7 @@ def main(args, config):
     config['pad_token_id'] = train_dataset.pad_token_id
     config['eos'] = train_dataset.eos_token
 
-    from models.model_generation import XVLMPlusForVQA
+    from benchmark.submodules.x2vlm.models.model_generation import XVLMPlusForVQA
     model = XVLMPlusForVQA(config=config)
     model.load_pretrained(args.checkpoint, config, is_eval=args.evaluate or args.load_vqa_pretrain)
     model = model.to(device)
diff --git a/XRetrieval.py b/XRetrieval.py
index 9c7a6c7..20f5faa 100644
--- a/XRetrieval.py
+++ b/XRetrieval.py
@@ -17,10 +17,10 @@ import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
 
-import utils
+import benchmark.submodules.x2vlm.utils as utils
 from dataset import create_dataset, create_sampler, create_loader, build_tokenizer
-from scheduler import create_scheduler
-from optim import create_optimizer
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler, config):
@@ -259,7 +259,7 @@ def main(args, config):
                                                num_workers=[4, 4], is_trains=[False, False], collate_fns=[None, None])
 
     print("Creating model", flush=True)
-    from models.model_retrieval import XVLMPlusForRetrieval
+    from benchmark.submodules.x2vlm.models.model_retrieval import XVLMPlusForRetrieval
     model = XVLMPlusForRetrieval(config=config)
     model.load_pretrained(args.checkpoint, config, is_eval=args.evaluate)
     model = model.to(device)
diff --git a/XVNLI.py b/XVNLI.py
index 48a69df..3c9283d 100644
--- a/XVNLI.py
+++ b/XVNLI.py
@@ -18,12 +18,12 @@ import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
 
-import utils
-from utils.hdfs_io import hexists, hmkdir
-from utils.checkpointer import Checkpointer
+import benchmark.submodules.x2vlm.utils as utils
+from benchmark.submodules.x2vlm.utils.hdfs_io import hexists, hmkdir
+from benchmark.submodules.x2vlm.utils.checkpointer import Checkpointer
 from dataset import create_dataset, create_sampler, create_loader, build_tokenizer
-from scheduler import create_scheduler
-from optim import create_optimizer
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler, config):
@@ -131,7 +131,7 @@ def main(args, config):
                                             num_workers=[4], is_trains=[False], collate_fns=[None])[0]
 
     print("Creating model")
-    from models.model_classification import XVLMPlus4XVNLI
+    from benchmark.submodules.x2vlm.models.model_classification import XVLMPlus4XVNLI
     model = XVLMPlus4XVNLI(config=config)
     model.load_pretrained(args.checkpoint, config, is_eval=args.evaluate)
     model = model.to(device)
diff --git a/dataset/__init__.py b/dataset/__init__.py
index fe6f79c..833857c 100644
--- a/dataset/__init__.py
+++ b/dataset/__init__.py
@@ -5,22 +5,22 @@ from torchvision import transforms
 
 from torchvision.transforms import InterpolationMode
 
-from dataset.tokenizers import build_tokenizer
-from dataset.pretrain_dataset import ImageTextJsonDataset, RegionTextJsonDataset, TextJsonDataset, FrameTextDataset
-from dataset.pretrain_dataset_multilingual import ImageMultiTextDataset, RegionMultiTextDataset, ParaTextDataset
-
-from dataset.retrieval_dataset import re_train_dataset, re_eval_dataset
-from dataset.nlvr_dataset import nlvr_dataset
-from dataset.vqa_dataset import vqa_dataset, msrvtt_qa_dataset, msvd_qa_dataset, vqa_classify_dataset
-from dataset.grounding_dataset import grounding_dataset, grounding_dataset_bbox
-from dataset.captioning_dataset import coco_karpathy_train, coco_karpathy_train_mlm, coco_karpathy_train_scst, coco_karpathy_caption_eval
-
-from dataset.vqa_dataset import xgqa_dataset, next_qa_mc_dataset
-from dataset.xvnli_dataset import xvnli_dataset
-from dataset.xflickrco_dataset import xflickrco_train_dataset, xflickrco_eval_dataset
-from dataset.wit_dataset import wit_train_dataset, wit_eval_dataset
-
-from dataset.randaugment import RandomAugment
+from benchmark.submodules.x2vlm.dataset.tokenizers import build_tokenizer
+from benchmark.submodules.x2vlm.dataset.pretrain_dataset import ImageTextJsonDataset, RegionTextJsonDataset, TextJsonDataset, FrameTextDataset
+from benchmark.submodules.x2vlm.dataset.pretrain_dataset_multilingual import ImageMultiTextDataset, RegionMultiTextDataset, ParaTextDataset
+
+from benchmark.submodules.x2vlm.dataset.retrieval_dataset import re_train_dataset, re_eval_dataset
+from benchmark.submodules.x2vlm.dataset.nlvr_dataset import nlvr_dataset
+from benchmark.submodules.x2vlm.dataset.vqa_dataset import vqa_dataset, msrvtt_qa_dataset, msvd_qa_dataset, vqa_classify_dataset
+from benchmark.submodules.x2vlm.dataset.grounding_dataset import grounding_dataset, grounding_dataset_bbox
+from benchmark.submodules.x2vlm.dataset.captioning_dataset import coco_karpathy_train, coco_karpathy_train_mlm, coco_karpathy_train_scst, coco_karpathy_caption_eval
+
+from benchmark.submodules.x2vlm.dataset.vqa_dataset import xgqa_dataset, next_qa_mc_dataset
+from benchmark.submodules.x2vlm.dataset.xvnli_dataset import xvnli_dataset
+from benchmark.submodules.x2vlm.dataset.xflickrco_dataset import xflickrco_train_dataset, xflickrco_eval_dataset
+from benchmark.submodules.x2vlm.dataset.wit_dataset import wit_train_dataset, wit_eval_dataset
+
+from benchmark.submodules.x2vlm.dataset.randaugment import RandomAugment
 
 
 def create_dataset(dataset, config, evaluate=False):
diff --git a/dataset/captioning_dataset.py b/dataset/captioning_dataset.py
index 6cd45fd..48973ec 100644
--- a/dataset/captioning_dataset.py
+++ b/dataset/captioning_dataset.py
@@ -10,9 +10,9 @@ from torchvision.datasets.utils import download_url
 
 from PIL import Image
 
-from dataset import build_tokenizer
-from dataset.utils import pre_caption
-from dataset.pretrain_dataset import TextMaskingGenerator
+from benchmark.submodules.x2vlm.dataset import build_tokenizer
+from benchmark.submodules.x2vlm.dataset.utils import pre_caption
+from benchmark.submodules.x2vlm.dataset.pretrain_dataset import TextMaskingGenerator
 
 
 class coco_karpathy_train(Dataset):
diff --git a/dataset/dist_dataset.py b/dataset/dist_dataset.py
index 3eb3566..b987c32 100644
--- a/dataset/dist_dataset.py
+++ b/dataset/dist_dataset.py
@@ -13,7 +13,7 @@ from itertools import cycle
 import torch
 from torch.utils.data import IterableDataset
 
-from utils.hdfs_io import hopen, hlist_files, hexists
+from benchmark.submodules.x2vlm.utils.hdfs_io import hopen, hlist_files, hexists
 
 
 class DistLineReadingDataset(IterableDataset):  # pylint: disable=W0223
diff --git a/dataset/grounding_dataset.py b/dataset/grounding_dataset.py
index 0fc6eb1..6f457d8 100644
--- a/dataset/grounding_dataset.py
+++ b/dataset/grounding_dataset.py
@@ -10,8 +10,8 @@ from torch.utils.data import Dataset
 from torchvision.transforms.functional import hflip, resize
 
 from PIL import Image
-from dataset.utils import pre_caption
-from refTools.refer_python3 import REFER
+from benchmark.submodules.x2vlm.dataset.utils import pre_caption
+from benchmark.submodules.x2vlm.refTools.refer_python3 import REFER
 
 
 class grounding_dataset(Dataset):
diff --git a/dataset/nlvr_dataset.py b/dataset/nlvr_dataset.py
index 705b8b6..9357736 100644
--- a/dataset/nlvr_dataset.py
+++ b/dataset/nlvr_dataset.py
@@ -2,7 +2,7 @@ import json
 import os
 from torch.utils.data import Dataset
 from PIL import Image
-from dataset.utils import pre_caption
+from benchmark.submodules.x2vlm.dataset.utils import pre_caption
 
 
 class nlvr_dataset(Dataset):
diff --git a/dataset/pretrain_dataset.py b/dataset/pretrain_dataset.py
index 350744e..1cebec9 100644
--- a/dataset/pretrain_dataset.py
+++ b/dataset/pretrain_dataset.py
@@ -28,9 +28,9 @@ from PIL import ImageFile
 ImageFile.LOAD_TRUNCATED_IMAGES = True
 Image.MAX_IMAGE_PIXELS = None
 
-from dataset import build_tokenizer
-from dataset.utils import pre_caption, sample_frame_ids, sample_clip_ids
-from dataset.dist_dataset import DistLineReadingDataset
+from benchmark.submodules.x2vlm.dataset import build_tokenizer
+from benchmark.submodules.x2vlm.dataset.utils import pre_caption, sample_frame_ids, sample_clip_ids
+from benchmark.submodules.x2vlm.dataset.dist_dataset import DistLineReadingDataset
 
 
 class TextMaskingGenerator:
diff --git a/dataset/pretrain_dataset_multilingual.py b/dataset/pretrain_dataset_multilingual.py
index 2c4f818..b1b1d65 100644
--- a/dataset/pretrain_dataset_multilingual.py
+++ b/dataset/pretrain_dataset_multilingual.py
@@ -29,9 +29,9 @@ from PIL import ImageFile
 ImageFile.LOAD_TRUNCATED_IMAGES = True
 Image.MAX_IMAGE_PIXELS = None
 
-from dataset import build_tokenizer
-from dataset.utils import pre_caption
-from dataset.dist_dataset import DistLineReadingDataset
+from benchmark.submodules.x2vlm.dataset import build_tokenizer
+from benchmark.submodules.x2vlm.dataset.utils import pre_caption
+from benchmark.submodules.x2vlm.dataset.dist_dataset import DistLineReadingDataset
 
 
 class TextMaskingGenerator:
diff --git a/dataset/retrieval_dataset.py b/dataset/retrieval_dataset.py
index 33593b6..ea4b6a1 100644
--- a/dataset/retrieval_dataset.py
+++ b/dataset/retrieval_dataset.py
@@ -13,7 +13,7 @@ from PIL import ImageFile
 ImageFile.LOAD_TRUNCATED_IMAGES = True
 Image.MAX_IMAGE_PIXELS = None
 
-from dataset.utils import pre_caption, sample_frame_ids
+from benchmark.submodules.x2vlm.dataset.utils import pre_caption, sample_frame_ids
 
 
 class re_train_dataset(Dataset):
diff --git a/dataset/tokenizers/__init__.py b/dataset/tokenizers/__init__.py
index 0ba029a..5b2d96f 100644
--- a/dataset/tokenizers/__init__.py
+++ b/dataset/tokenizers/__init__.py
@@ -1,5 +1,5 @@
 from transformers import BertTokenizer, RobertaTokenizer, XLMRobertaTokenizer, AutoTokenizer
-from dataset.tokenizers.bert_tokenizer_with_dropout import BertTokenizerWithDropout
+from benchmark.submodules.x2vlm.dataset.tokenizers.bert_tokenizer_with_dropout import BertTokenizerWithDropout
 
 
 def build_tokenizer(text_encoder: str, dropout=0):
diff --git a/dataset/utils.py b/dataset/utils.py
index 730efc0..db6cbf1 100644
--- a/dataset/utils.py
+++ b/dataset/utils.py
@@ -8,12 +8,12 @@ import torch.distributed as dist
 import torch.nn.functional as F
 import pickle
 
-import utils
+import benchmark.submodules.x2vlm.utils as utils
 from tqdm import tqdm
 
-from utils.hdfs_io import hexists, hcopy, hopen
-from vqaTools.vqaEval import VQAEval
-from refTools.evaluation.refEvaluation import RefEvaluation
+from benchmark.submodules.x2vlm.utils.hdfs_io import hexists, hcopy, hopen
+from benchmark.submodules.x2vlm.vqaTools.vqaEval import VQAEval
+from benchmark.submodules.x2vlm.refTools.evaluation.refEvaluation import RefEvaluation
 
 
 def sample_clip_ids(clips, mininum_frames: int, clip_captions=None, skip_caption_set=None):
diff --git a/dataset/vqa_dataset.py b/dataset/vqa_dataset.py
index 3ee1b31..14a1227 100644
--- a/dataset/vqa_dataset.py
+++ b/dataset/vqa_dataset.py
@@ -9,10 +9,10 @@ import torch
 
 from PIL import Image
 from torch.utils.data import Dataset
-from dataset.utils import pre_question, sample_frame_ids
+from benchmark.submodules.x2vlm.dataset.utils import pre_question, sample_frame_ids
 
 from torchvision.transforms.functional import hflip
-from dataset import build_tokenizer
+from benchmark.submodules.x2vlm.dataset import build_tokenizer
 
 
 class vqa_dataset(Dataset):
diff --git a/dataset/wit_dataset.py b/dataset/wit_dataset.py
index 74413ff..12ac270 100644
--- a/dataset/wit_dataset.py
+++ b/dataset/wit_dataset.py
@@ -18,7 +18,7 @@ from tqdm import tqdm
 ImageFile.LOAD_TRUNCATED_IMAGES = True
 Image.MAX_IMAGE_PIXELS = None
 
-from dataset.utils import pre_caption
+from benchmark.submodules.x2vlm.dataset.utils import pre_caption
 
 
 class wit_train_dataset(Dataset):
diff --git a/dataset/xflickrco_dataset.py b/dataset/xflickrco_dataset.py
index 05a67ed..5a20abc 100644
--- a/dataset/xflickrco_dataset.py
+++ b/dataset/xflickrco_dataset.py
@@ -14,7 +14,7 @@ from PIL import ImageFile
 ImageFile.LOAD_TRUNCATED_IMAGES = True
 Image.MAX_IMAGE_PIXELS = None
 
-from dataset.utils import pre_caption
+from benchmark.submodules.x2vlm.dataset.utils import pre_caption
 
 
 class xflickrco_train_dataset(Dataset):
diff --git a/dataset/xvnli_dataset.py b/dataset/xvnli_dataset.py
index 145e4c3..c476812 100644
--- a/dataset/xvnli_dataset.py
+++ b/dataset/xvnli_dataset.py
@@ -7,7 +7,7 @@ import json
 import os
 from torch.utils.data import Dataset
 from PIL import Image
-from dataset.utils import pre_caption
+from benchmark.submodules.x2vlm.dataset.utils import pre_caption
 
 
 class xvnli_dataset(Dataset):
diff --git a/models/__init__.py b/models/__init__.py
index 992c571..ab732dd 100644
--- a/models/__init__.py
+++ b/models/__init__.py
@@ -1,3 +1,3 @@
-from models.xvlm import XVLMBase
-from models.xvlm import build_mlp
-from models.xvlm import load_pretrained
\ No newline at end of file
+from .xvlm import XVLMBase
+from .xvlm import build_mlp
+from .xvlm import load_pretrained
\ No newline at end of file
diff --git a/models/model_classification.py b/models/model_classification.py
index bd4bd1e..42aa887 100644
--- a/models/model_classification.py
+++ b/models/model_classification.py
@@ -9,8 +9,8 @@ from torch.nn import MSELoss
 
 from einops import rearrange
 
-from models.xvlm import XVLMBase, XVLMPlusBase
-from models.xvlm import build_mlp
+from benchmark.submodules.x2vlm.models.xvlm import XVLMBase, XVLMPlusBase
+from benchmark.submodules.x2vlm.models.xvlm import build_mlp
 
 
 class XVLMForClassification(XVLMBase):
diff --git a/models/model_grounding.py b/models/model_grounding.py
index 509e18b..8f2ea30 100644
--- a/models/model_grounding.py
+++ b/models/model_grounding.py
@@ -1,4 +1,4 @@
-from models import XVLMBase, load_pretrained
+from benchmark.submodules.x2vlm.models import XVLMBase, load_pretrained
 
 
 class XVLMForGrounding(XVLMBase):
diff --git a/models/model_pretrain.py b/models/model_pretrain.py
index ca66ad9..86c3a9d 100644
--- a/models/model_pretrain.py
+++ b/models/model_pretrain.py
@@ -18,7 +18,7 @@ import json
 import torch
 from einops import rearrange
 
-from models.xvlm import XVLMBase, XVLMPlusBase, VanillaConfig
+from benchmark.submodules.x2vlm.models.xvlm import XVLMBase, XVLMPlusBase, VanillaConfig
 
 
 class XVLM(XVLMBase):
diff --git a/models/model_retrieval.py b/models/model_retrieval.py
index 552fbb6..d866882 100644
--- a/models/model_retrieval.py
+++ b/models/model_retrieval.py
@@ -1,6 +1,6 @@
 import torch
 import torch.nn.functional as F
-from models.xvlm import XVLMBase, XVLMPlusBase
+from benchmark.submodules.x2vlm.models.xvlm import XVLMBase, XVLMPlusBase
 
 
 class XVLMForRetrieval(XVLMBase):
diff --git a/models/xroberta.py b/models/xroberta.py
index 0da8874..4dd1de5 100644
--- a/models/xroberta.py
+++ b/models/xroberta.py
@@ -1103,7 +1103,28 @@ class RobertaForCausalLM(RobertaPreTrainedModel):
 
 @dataclass
 class MaskedLMOutput(ModelOutput):
+    """
+    Base class for outputs of masked language modeling models.
+
+    Args:
+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
+            Language modeling (masked LM) loss.
+
+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, vocab_size)`):
+            Prediction scores of the language modeling head (before softmax).
+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, + one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
 
+            Hidden-states of the model at the output of each layer plus the initial embedding outputs, if applicable.
+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
+
+            Attention weights after the attention softmax, used to compute the weighted average in the self-attention heads.
+        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):
+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
+
+            Attention weights of the decoder's cross-attention layer, after the attention softmax, used to compute the weighted average in the cross-attention heads.
+    """
     loss: Optional[torch.FloatTensor] = None
     logits: torch.FloatTensor = None
     hidden_states: Optional[Tuple[torch.FloatTensor]] = None
diff --git a/models/xvlm.py b/models/xvlm.py
index fabb297..d945507 100644
--- a/models/xvlm.py
+++ b/models/xvlm.py
@@ -19,15 +19,14 @@ from torch.nn import CrossEntropyLoss
 from einops import rearrange
 
 from timm.models.layers import trunc_normal_
+from benchmark.submodules.x2vlm.models import box_ops
 
-from models import box_ops
-
-from models.xbert import BertConfig, BertForMaskedLM, BertModel
-from models.xroberta import RobertaForMaskedLM, RobertaModel, RobertaConfig
+from benchmark.submodules.x2vlm.models.xbert import BertConfig, BertForMaskedLM, BertModel
+from benchmark.submodules.x2vlm.models.xroberta import RobertaForMaskedLM, RobertaModel, RobertaConfig
 import copy
 
-from utils import read_json
-from dataset import build_tokenizer
+from benchmark.submodules.x2vlm.utils import read_json
+from benchmark.submodules.x2vlm.dataset import build_tokenizer
 
 
 class VanillaConfig(object):
@@ -177,7 +176,7 @@ def build_vision_encoder(config, load_params=False):
     num_patches = (config['image_res'] // config['patch_size']) ** 2
 
     if config.get('use_clip_vit', False):  # good performance, but only base model available
-        from models.clip_vit import CLIPVisionTransformer, interpolate_pos_embed
+        from benchmark.submodules.x2vlm.models.clip_vit import CLIPVisionTransformer, interpolate_pos_embed
 
         vision_config = read_json(config['vision_config'])
         assert config['patch_size'] == vision_config['patch_size']
@@ -217,7 +216,7 @@ def build_vision_encoder(config, load_params=False):
                 load_params_choose_layers('encoder.layers', state_dict, mapper)
 
     elif config.get('use_swin', False):
-        from models.swin_transformer import SwinTransformer
+        from benchmark.submodules.x2vlm.models.swin_transformer import SwinTransformer
 
         vision_config = read_json(config['vision_config'])
         assert config['image_res'] == vision_config['image_res']
@@ -240,7 +239,7 @@ def build_vision_encoder(config, load_params=False):
                                          use_checkpoint=False, add_cls=config.get('swin_add_cls', True))
 
         if load_params:
-            from models.swin_transformer import load_pretrained_swin
+            from benchmark.submodules.x2vlm.models.swin_transformer import load_pretrained_swin
             state_dict = load_pretrained_swin(vision_encoder, vision_config['ckpt'])
 
     elif config.get('use_beit_v2', False):
@@ -250,9 +249,9 @@ def build_vision_encoder(config, load_params=False):
         vision_width = vision_config['vision_width']
 
         if 'base' in config['vision_config']:
-            from models.beit2 import beit_base_patch16 as beit_model
+            from benchmark.submodules.x2vlm.models.beit2 import beit_base_patch16 as beit_model
         elif 'large' in config['vision_config']:
-            from models.beit2 import beit_large_patch16 as beit_model
+            from benchmark.submodules.x2vlm.models.beit2 import beit_large_patch16 as beit_model
         else:
             raise ValueError
 
@@ -265,7 +264,7 @@ def build_vision_encoder(config, load_params=False):
                                     vision_num_hidden_layers=config.get('vision_num_hidden_layers', -1))
 
         if load_params:
-            from models.beit2 import load_pretrained_beit2
+            from benchmark.submodules.x2vlm.models.beit2 import load_pretrained_beit2
             load_pretrained_beit2(vision_encoder, vision_config['ckpt'])
 
     else:
@@ -397,7 +396,7 @@ def load_pretrained(model, ckpt_rpath, config, is_eval=False, load_text=False):
     print("### Loading pretrained vision encoder", flush=True)
 
     if config.get('use_clip_vit', False):
-        from models.clip_vit import interpolate_pos_embed
+        from benchmark.submodules.x2vlm.models.clip_vit import interpolate_pos_embed
         del state_dict['vision_encoder.position_ids']
         num_patches = (config['image_res'] // config['patch_size']) ** 2
         pos_embed_reshaped = interpolate_pos_embed(state_dict['vision_encoder.pos_embed.weight'].unsqueeze(dim=0),
@@ -405,7 +404,7 @@ def load_pretrained(model, ckpt_rpath, config, is_eval=False, load_text=False):
         state_dict['vision_encoder.pos_embed.weight'] = pos_embed_reshaped.squeeze(dim=0)
 
     elif config.get('use_swin', False) or config.get('use_swin_v2', False):
-        from models.swin_transformer import load_pretrained_swin
+        from benchmark.submodules.x2vlm.models.swin_transformer import load_pretrained_swin
 
         vision_state_dict = {}
         for k in list(state_dict.keys()):
@@ -419,7 +418,7 @@ def load_pretrained(model, ckpt_rpath, config, is_eval=False, load_text=False):
             state_dict['vision_encoder.' + k] = vision_state_dict[k]
 
     elif config.get('use_beit_v2', False):
-        from models.beit2 import interpolate_pos_embed
+        from benchmark.submodules.x2vlm.models.beit2 import interpolate_pos_embed
 
         vision_state_dict = {}
         for k in list(state_dict.keys()):
@@ -972,7 +971,7 @@ class XVLMPlusBase(XVLMBase):
 
         self.use_mlm_loss = use_mlm_loss
         if use_mlm_loss:
-            from models.xbert import BertOnlyMLMHead
+            from benchmark.submodules.x2vlm.models.xbert import BertOnlyMLMHead
             self.mlm_head = BertOnlyMLMHead(self.text_encoder.config)
             self.update_init_params(['mlm_head.' + n for (n, _) in self.mlm_head.named_parameters()])
             # self.tie_text_and_cross_wordemb()
diff --git a/refTools/evaluation/bleu/bleu.py b/refTools/evaluation/bleu/bleu.py
index 1dab5ad..58c9734 100644
--- a/refTools/evaluation/bleu/bleu.py
+++ b/refTools/evaluation/bleu/bleu.py
@@ -8,7 +8,7 @@
 # Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT
 # Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>
 
-from refTools.evaluation.bleu.bleu_scorer import BleuScorer
+from benchmark.submodules.x2vlm.refTools.evaluation.bleu.bleu_scorer import BleuScorer
 
 
 class Bleu:
diff --git a/refTools/evaluation/cider/cider.py b/refTools/evaluation/cider/cider.py
index c96f4f4..b173f34 100644
--- a/refTools/evaluation/cider/cider.py
+++ b/refTools/evaluation/cider/cider.py
@@ -7,7 +7,7 @@
 #
 # Authors: Ramakrishna Vedantam <vrama91@vt.edu> and Tsung-Yi Lin <tl483@cornell.edu>
 
-from refTools.evaluation.cider.cider_scorer import CiderScorer
+from benchmark.submodules.x2vlm.refTools.evaluation.cider.cider_scorer import CiderScorer
 import pdb
 
 class Cider:
diff --git a/refTools/evaluation/refEvaluation.py b/refTools/evaluation/refEvaluation.py
index 8045cb5..cb1203d 100644
--- a/refTools/evaluation/refEvaluation.py
+++ b/refTools/evaluation/refEvaluation.py
@@ -1,8 +1,8 @@
-from refTools.evaluation.tokenizer.ptbtokenizer import PTBTokenizer
-from refTools.evaluation.bleu.bleu import Bleu
-from refTools.evaluation.meteor.meteor import Meteor
-from refTools.evaluation.rouge.rouge import Rouge
-from refTools.evaluation.cider.cider import Cider
+from benchmark.submodules.x2vlm.refTools.evaluation.tokenizer.ptbtokenizer import PTBTokenizer
+from benchmark.submodules.x2vlm.refTools.evaluation.bleu.bleu import Bleu
+from benchmark.submodules.x2vlm.refTools.evaluation.meteor.meteor import Meteor
+from benchmark.submodules.x2vlm.refTools.evaluation.rouge.rouge import Rouge
+from benchmark.submodules.x2vlm.refTools.evaluation.cider.cider import Cider
 
 """
 Input: refer and Res = [{ref_id, sent}]
diff --git a/run.py b/run.py
index 9a68d13..88daebf 100644
--- a/run.py
+++ b/run.py
@@ -9,7 +9,7 @@ import time
 import random
 import argparse
 
-from utils.hdfs_io import HADOOP_BIN, hexists, hmkdir, hcopy
+from benchmark.submodules.x2vlm.utils.hdfs_io import HADOOP_BIN, hexists, hmkdir, hcopy
 
 ############ Set it correctly for distributed training across nodes
 NNODES = int(os.getenv("ARNOLD_WORKER_NUM"))  # e.g. 1/2/3/4
@@ -311,7 +311,7 @@ def run_marvl(args):
     dist_launch = get_dist_launch(args)
 
     if not os.path.exists('data_mm/marvl'):
-        from utils.marvl_preproc import marvl_preproc
+        from benchmark.submodules.x2vlm.utils.marvl_preproc import marvl_preproc
         marvl_preproc('iglue/datasets/marvl', 'data_mm/marvl')
 
     assert os.path.exists("images/nlvr2")
diff --git a/utils/__init__.py b/utils/__init__.py
index c7646c9..183ee93 100644
--- a/utils/__init__.py
+++ b/utils/__init__.py
@@ -10,7 +10,7 @@ import numpy as np
 import torch
 import torch.distributed as dist
 
-from utils.cider.pyciderevalcap.ciderD.ciderD import CiderD
+from benchmark.submodules.x2vlm.utils.cider.pyciderevalcap.ciderD.ciderD import CiderD
 import ruamel.yaml as yaml
 
 
diff --git a/utils/checkpointer.py b/utils/checkpointer.py
index f4b6d9d..7e07f25 100644
--- a/utils/checkpointer.py
+++ b/utils/checkpointer.py
@@ -11,8 +11,8 @@ import time
 
 import torch
 
-from utils.hdfs_io import hexists, hmkdir, hcopy
-from utils.torch_io import save as hdfs_torch_save
+from benchmark.submodules.x2vlm.utils.hdfs_io import hexists, hmkdir, hcopy
+from benchmark.submodules.x2vlm.utils.torch_io import save as hdfs_torch_save
 logger = logging.getLogger(__name__)
 
 
diff --git a/xFlickrCO.py b/xFlickrCO.py
index 85ebec5..278f4dc 100644
--- a/xFlickrCO.py
+++ b/xFlickrCO.py
@@ -16,11 +16,11 @@ import torch.nn.functional as F
 import torch.backends.cudnn as cudnn
 import torch.distributed as dist
 
-import utils
-from utils.hdfs_io import hopen, hexists, hmkdir
+import benchmark.submodules.x2vlm.utils as utils
+from benchmark.submodules.x2vlm.utils.hdfs_io import hopen, hexists, hmkdir
 from dataset import create_dataset, create_sampler, create_loader, build_tokenizer
-from scheduler import create_scheduler
-from optim import create_optimizer
+from benchmark.submodules.x2vlm.scheduler import create_scheduler
+from benchmark.submodules.x2vlm.optim import create_optimizer
 
 
 def train(model, data_loader, optimizer, tokenizer, epoch, device, scheduler, config):
@@ -248,7 +248,7 @@ def main(args, config):
                                             num_workers=[4], is_trains=[False], collate_fns=[None])[0]
 
     print("Creating model", flush=True)
-    from models.model_retrieval import XVLMPlusForRetrieval
+    from benchmark.submodules.x2vlm.models.model_retrieval import XVLMPlusForRetrieval
     model = XVLMPlusForRetrieval(config=config)
     model.load_pretrained(args.checkpoint, config, is_eval=args.evaluate)
     model = model.to(device)
